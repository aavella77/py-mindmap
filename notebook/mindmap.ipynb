{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from wikipedia import search, page\n",
    "from gensim.parsing import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "from networkx import Graph\n",
    "from collections import defaultdict\n",
    "import pygraphviz\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_stemmer = PorterStemmer()\n",
    " \n",
    "class StemmingHelper(object):\n",
    "    \"\"\"\n",
    "    Class to aid the stemming process - from word to stemmed form,\n",
    "    and vice versa.\n",
    "    The 'original' form of a stemmed word will be returned as the\n",
    "    form in which its been used the most number of times in the text.\n",
    "    \"\"\"\n",
    " \n",
    "    #This reverse lookup will remember the original forms of the stemmed\n",
    "    #words\n",
    "    word_lookup = {}\n",
    " \n",
    "    @classmethod\n",
    "    def stem(cls, word):\n",
    "        \"\"\"\n",
    "        Stems a word and updates the reverse lookup.\n",
    "        \"\"\"\n",
    " \n",
    "        #Stem the word\n",
    "        stemmed = global_stemmer.stem(word)\n",
    " \n",
    "        #Update the word lookup\n",
    "        if stemmed not in cls.word_lookup:\n",
    "            cls.word_lookup[stemmed] = {}\n",
    "        cls.word_lookup[stemmed][word] = (\n",
    "            cls.word_lookup[stemmed].get(word, 0) + 1)\n",
    " \n",
    "        return stemmed\n",
    " \n",
    "    @classmethod\n",
    "    def original_form(cls, word):\n",
    "        \"\"\"\n",
    "        Returns original form of a word given the stemmed version,\n",
    "        as stored in the word lookup.\n",
    "        \"\"\"\n",
    " \n",
    "        if word in cls.word_lookup:\n",
    "            return max(cls.word_lookup[word].keys(),\n",
    "                       key=lambda x: cls.word_lookup[word][x])\n",
    "        else:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _get_param_matrices(vocabulary, sentence_terms):\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    =======\n",
    "    1. Top 300(or lesser, if vocab is short) most frequent terms(list)\n",
    "    2. co-occurence matrix wrt the most frequent terms(dict)\n",
    "    3. Dict containing Pg of most-frequent terms(dict)\n",
    "    4. nw(no of terms affected) of each term(dict)\n",
    "    \"\"\"\n",
    " \n",
    "    #Figure out top n terms with respect to mere occurences\n",
    "    n = min(300, len(vocabulary))\n",
    "    topterms = list(vocabulary.keys())\n",
    "    topterms.sort(key = lambda x: vocabulary[x], reverse = True)\n",
    "    topterms = topterms[:n]\n",
    " \n",
    "    #nw maps term to the number of terms it 'affects'\n",
    "    #(sum of number of terms in all sentences it\n",
    "    #appears in)\n",
    "    nw = {}\n",
    "    #Co-occurence values are wrt top terms only\n",
    "    co_occur = {}\n",
    "    #Initially, co-occurence matrix is empty\n",
    "    for x in vocabulary:\n",
    "        co_occur[x] = [0 for i in range(len(topterms))]\n",
    " \n",
    "    #Iterate over list of all sentences' vocabulary dictionaries\n",
    "    #Build the co-occurence matrix\n",
    "    for sentence in sentence_terms:\n",
    "        total_terms = sum(list(sentence.values()))\n",
    "        #This list contains the indices of all terms from topterms,\n",
    "        #that are present in this sentence\n",
    "        top_indices = []\n",
    "        #Populate top_indices\n",
    "        top_indices = [topterms.index(x) for x in sentence\n",
    "                       if x in topterms]\n",
    "        #Update nw dict, and co-occurence matrix\n",
    "        for term in sentence:\n",
    "            nw[term] = nw.get(term, 0) + total_terms\n",
    "            for index in top_indices:\n",
    "                co_occur[term][index] += (sentence[term] *\n",
    "                                          sentence[topterms[index]])\n",
    " \n",
    "    #Pg is just nw[term]/total vocabulary of text\n",
    "    Pg = {}\n",
    "    N = sum(list(vocabulary.values()))\n",
    "    for x in topterms:\n",
    "        Pg[x] = float(nw[x])/N\n",
    " \n",
    "    return topterms, co_occur, Pg, nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_n_terms(vocabulary, sentence_terms, n=50):\n",
    "    \"\"\"\n",
    "    Returns the top 'n' terms from a block of text, in the form of a list,\n",
    "    from most important to least.\n",
    " \n",
    "    'vocabulary' should be a dict mapping each term to the number\n",
    "    of its occurences in the entire text.\n",
    "    'sentence_terms' should be an iterable of dicts, each denoting the\n",
    "    vocabulary of the corresponding sentence.\n",
    "    \"\"\"\n",
    " \n",
    "    #First compute the matrices\n",
    "    topterms, co_occur, Pg, nw = _get_param_matrices(vocabulary,\n",
    "                                                     sentence_terms)\n",
    " \n",
    "    #This dict will map each term to its weightage with respect to the\n",
    "    #document\n",
    "    result = {}\n",
    " \n",
    "    N = sum(list(vocabulary.values()))\n",
    "    #Iterates over all terms in vocabulary\n",
    "    for term in co_occur:\n",
    "        term = str(term)\n",
    "        org_term = str(term)\n",
    "        for x in Pg:\n",
    "            #expected_cooccur is the expected cooccurence of term with this\n",
    "            #term, based on nw value of this and Pg value of the other\n",
    "            expected_cooccur = nw[term] * Pg[x]\n",
    "            #Result measures the difference(in no of terms) of expected\n",
    "            #cooccurence and  actual cooccurence\n",
    "            result[org_term] = ((co_occur[term][topterms.index(x)] -\n",
    "                                 expected_cooccur)**2/ float(expected_cooccur))\n",
    " \n",
    "    terms = list(result.keys())\n",
    "    terms.sort(key=lambda x: result[x],\n",
    "               reverse=True)\n",
    " \n",
    "    return terms[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mind_map(model, stemmer, root, nodes, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Returns the Mind-Map in the form of a NetworkX Graph instance.\n",
    " \n",
    "    'model' should be an instance of gensim.models.Word2Vec\n",
    "    'nodes' should be a list of terms, included in the vocabulary of\n",
    "    'model'.\n",
    "    'root' should be the node that is to be used as the root of the Mind\n",
    "    Map graph.\n",
    "    'stemmer' should be an instance of StemmingHelper.\n",
    "    \"\"\"\n",
    " \n",
    "    #This will be the Mind-Map\n",
    "    g = nx.DiGraph()\n",
    " \n",
    "    #Ensure that the every node is in the vocabulary of the Word2Vec\n",
    "    #model, and that the root itself is included in the given nodes\n",
    "    for node in nodes:\n",
    "        if node not in model.vocab:\n",
    "            raise ValueError(node + \" not in model's vocabulary\")\n",
    "    if root not in nodes:\n",
    "        raise ValueError(\"root not in nodes\")\n",
    " \n",
    "    ##Containers for algorithm run\n",
    "    #Initially, all nodes are unvisited\n",
    "    unvisited_nodes = set(nodes)\n",
    "    #Initially, no nodes are visited\n",
    "    visited_nodes = set([])\n",
    "    #The following will map visited node to its contextual vector\n",
    "    visited_node_vectors = {}\n",
    "    #Thw following will map unvisited nodes to (closest_distance, parent)\n",
    "    #parent will obviously be a visited node\n",
    "    node_distances = {}\n",
    " \n",
    "    #Initialization with respect to root\n",
    "    current_node = root\n",
    "    visited_node_vectors[root] = model[root]\n",
    "    unvisited_nodes.remove(root)\n",
    "    visited_nodes.add(root)\n",
    " \n",
    "    #Build the Mind-Map in n-1 iterations\n",
    "    for i in range(1, len(nodes)):\n",
    "        #For every unvisited node 'x'\n",
    "        for x in unvisited_nodes:\n",
    "            #Compute contextual distance between current node and x\n",
    "            dist_from_current = cosine(visited_node_vectors[current_node],\n",
    "                                       model[x])\n",
    "            #Get the least contextual distance to x found until now\n",
    "            distance = node_distances.get(x, (100, ''))\n",
    "            #If current node provides a shorter path to x, update x's\n",
    "            #distance and parent information\n",
    "            if distance[0] > dist_from_current:\n",
    "                node_distances[x] = (dist_from_current, current_node)\n",
    " \n",
    "        #Choose next 'current' as that unvisited node, which has the\n",
    "        #lowest contextual distance from any of the visited nodes\n",
    "        next_node = min(unvisited_nodes,\n",
    "                        key=lambda x: node_distances[x][0])\n",
    " \n",
    "        ##Update all containers\n",
    "        parent = node_distances[next_node][1]\n",
    "        del node_distances[next_node]\n",
    "        next_node_vect = ((1 - alpha)*model[next_node] +\n",
    "                          alpha*visited_node_vectors[parent])\n",
    "        visited_node_vectors[next_node] = next_node_vect\n",
    "        unvisited_nodes.remove(next_node)\n",
    "        visited_nodes.add(next_node)\n",
    " \n",
    "        #Add the link between newly selected node and its parent(from the\n",
    "        #visited nodes) to the NetworkX Graph instance\n",
    "        g.add_edge(stemmer.original_form(parent).capitalize(),\n",
    "                   stemmer.original_form(next_node).capitalize())\n",
    " \n",
    "        #The new node becomes the current node for the next iteration\n",
    "        current_node = next_node\n",
    " \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decorate(decorator):\n",
    "    '''\n",
    "    Decorator wrapper - decorator(func(text))\n",
    "    '''\n",
    "    def decoratorFn(func):\n",
    "        def wrap(text):\n",
    "            return decorator(func(text))\n",
    "        return wrap\n",
    "    return decoratorFn\n",
    "\n",
    "def loop(decorator):\n",
    "    '''\n",
    "    Decorator wrapper - [decorator(item) for item in func(text)]\n",
    "    '''\n",
    "    def decoratorFn(func):\n",
    "        def wrap(text):\n",
    "            return [decorator(item) for item in func(text)]\n",
    "        return wrap\n",
    "    return decoratorFn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removePunctuations(text):\n",
    "    return re.sub(r'[^a-z ]', '', text).strip()\n",
    "\n",
    "def padConnectors(text):\n",
    "    return re.sub(r'[,:\\(\\)\\{\\}\\[\\]]{1,}', ' ', text).strip()\n",
    "\n",
    "def removeExtraSpaces(text):\n",
    "    return re.sub(r' {1,}', ' ', text).strip()\n",
    "\n",
    "def filterStopwords(text):\n",
    "    return filter(lambda x: len(x) >= 3, text)\n",
    "\n",
    "@loop(StemmingHelper.stem)\n",
    "@decorate(filterStopwords)\n",
    "def textToTokens(sent):\n",
    "    return sent.strip().split(' ')\n",
    "\n",
    "@decorate(removePunctuations)\n",
    "@decorate(removeExtraSpaces)\n",
    "@decorate(padConnectors)\n",
    "def cleanLine(text):\n",
    "    return text\n",
    "    \n",
    "@loop(textToTokens)\n",
    "@loop(cleanLine)\n",
    "def textToSents(text):\n",
    "    return re.split(r'[\\r|\\n|\\.|;]{1,}',text.strip())\n",
    "\n",
    "@decorate(textToSents)\n",
    "def parser(text):\n",
    "    return text.strip().lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keepOnly(bow, vocab):\n",
    "    return {key:val for (key,val) in bow.items() if vocab.get(key)}\n",
    "\n",
    "\n",
    "def sentToBow(sent, tf = None):\n",
    "    if tf is None:\n",
    "        tf = defaultdict(int)\n",
    "    for token in sent:\n",
    "        tf[token] += 1\n",
    "    return tf\n",
    "\n",
    "@loop(sentToBow)\n",
    "def sentsToBow(sents):\n",
    "    return sents\n",
    "\n",
    "def sentsToGlobalBow(sents):\n",
    "    tf = defaultdict(int)\n",
    "    for sent in sents:\n",
    "        sentToBow(sent, tf)\n",
    "    return tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('default_stopwords.txt') as f:\n",
    "    stopwords = f.read().strip().splitlines()\n",
    "    stopwords = list(map(lambda x: StemmingHelper.stem(x), stopwords))\n",
    "\n",
    "def removeStopwords(sents, stopwords = stopwords):\n",
    "    return [[token for token in sent if token not in stopwords] for sent in sents]\n",
    "    \n",
    "def wikiSource(theme = 'machine learning'):\n",
    "    titles = search(theme)\n",
    "    wikipage = page(titles[0])\n",
    "    return wikipage.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('RSRN.csv')\n",
    "raw = '\\n'.join(df.text.tolist())\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "size = 50\n",
    "window = 4\n",
    "\n",
    "#sents = parser(wikiSource('machine learning'))\n",
    "sents = parser(raw)\n",
    "sents = removeStopwords(sents)\n",
    "model = Word2Vec(sents, min_count=min_count, size=size, window=window)\n",
    "vocab = model.vocab\n",
    "sentBow = [keepOnly(bow, vocab) for bow in sentsToBow(sents)]\n",
    "globalBow = keepOnly(sentsToGlobalBow(sents), vocab)\n",
    "\n",
    "nodes = get_top_n_terms(globalBow, sentBow, 25)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mm = build_mind_map(model, StemmingHelper, 'nathan', nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (30, 30)\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "def graph_draw(graph):\n",
    "    pos=graphviz_layout(graph)\n",
    "    nx.draw_networkx_nodes(graph,\n",
    "            pos,\n",
    "            node_size=3000,\n",
    "            node_color=\"w\")\n",
    "    \n",
    "    nx.draw_networkx_labels(graph, pos, font_size=16)\n",
    "    nx.draw_networkx_edges(graph, pos)\n",
    "    plt.show()\n",
    "    \n",
    "graph_draw(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mm.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[StemmingHelper.original_form(token) for token in nodes]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
